{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esNWMzKrVuWc"
      },
      "source": [
        "**Revised on 3/5/2024: Changed source files**\n",
        "\n",
        "This is the skeleton code for Task 1 of the midterm project. The files that are downloaded in step 4 are based on the [Ember 2018 dataset](https://arxiv.org/abs/1804.04637), and contain the features (and corresponding labels) extracted from 1 million PE files, split into 80\\% training and 20\\% test datasets. The code used for for feature extraction is available [here](https://colab.research.google.com/drive/16q9bOlCmnTquPtVXVzxUj4ZY1ORp10R2?usp=sharing). However, the preprocessing and featurization process may take up to 3 hours on Google Colab. Hence, I recommend using the processed datasets (Step 4) to speed up your development.\n",
        "\n",
        "Also, note that there is a new optional step 8.5 - To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets.\n",
        "\n",
        "**Step 1:** Mount your Google Drive by clicking on \"Mount Drive\" in the Files section (panel to the left of this text.)\n",
        "\n",
        "**Step 2:** Go to Runtime -> Change runtime type and select T4 GPU.\n",
        "\n",
        "**Step 3:** Create a folder in your Google Drive, and rename it to \"vMalConv\"\n",
        "\n",
        "**Step 4:** Download the pre-processed training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUq_FZwmZegw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd12c11-542e-44a0-a01d-f0e7599cf0b4"
      },
      "source": [
        "# ~8GB\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-12 03:00:34--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 3.5.25.193, 52.217.139.233, 52.217.98.36, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|3.5.25.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7619200000 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘X_train.dat’\n",
            "\n",
            "X_train.dat         100%[===================>]   7.10G  16.0MB/s    in 8m 11s  \n",
            "\n",
            "2024-03-12 03:08:46 (14.8 MB/s) - ‘X_train.dat’ saved [7619200000/7619200000]\n",
            "\n",
            "--2024-03-12 03:08:46--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 3.5.19.125, 52.217.162.33, 52.216.25.228, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|3.5.19.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1904800000 (1.8G) [binary/octet-stream]\n",
            "Saving to: ‘X_test.dat’\n",
            "\n",
            "X_test.dat          100%[===================>]   1.77G  16.3MB/s    in 2m 2s   \n",
            "\n",
            "2024-03-12 03:10:49 (14.9 MB/s) - ‘X_test.dat’ saved [1904800000/1904800000]\n",
            "\n",
            "--2024-03-12 03:10:49--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 3.5.28.247, 3.5.25.66, 52.216.51.9, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|3.5.28.247|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200000 (3.1M) [binary/octet-stream]\n",
            "Saving to: ‘y_train.dat’\n",
            "\n",
            "y_train.dat         100%[===================>]   3.05M   785KB/s    in 4.5s    \n",
            "\n",
            "2024-03-12 03:10:54 (690 KB/s) - ‘y_train.dat’ saved [3200000/3200000]\n",
            "\n",
            "--2024-03-12 03:10:54--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 54.231.137.249, 52.216.50.185, 52.216.221.25, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|54.231.137.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800000 (781K) [binary/octet-stream]\n",
            "Saving to: ‘y_test.dat’\n",
            "\n",
            "y_test.dat          100%[===================>] 781.25K   862KB/s    in 0.9s    \n",
            "\n",
            "2024-03-12 03:10:56 (862 KB/s) - ‘y_test.dat’ saved [800000/800000]\n",
            "\n",
            "--2024-03-12 03:10:56--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 54.231.137.249, 52.216.50.185, 52.216.221.25, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|54.231.137.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92160330 (88M) [text/csv]\n",
            "Saving to: ‘metadata.csv’\n",
            "\n",
            "metadata.csv        100%[===================>]  87.89M  16.6MB/s    in 6.6s    \n",
            "\n",
            "2024-03-12 03:11:04 (13.4 MB/s) - ‘metadata.csv’ saved [92160330/92160330]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ_JdZKfG7Q-",
        "outputId": "792ff0ad-d707-4331-bdeb-e483ab766f50"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V958PbDW3H0"
      },
      "source": [
        "**Step 5:** Copy the downloaded files to vMalConv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llip77F3amma"
      },
      "source": [
        "!cp /content/X_train.dat /content/drive/MyDrive/vMalConv/X_train.dat\n",
        "!cp /content/X_test.dat /content/drive/MyDrive/vMalConv/X_test.dat\n",
        "!cp /content/y_train.dat /content/drive/MyDrive/vMalConv/y_train.dat\n",
        "!cp /content/y_test.dat /content/drive/MyDrive/vMalConv/y_test.dat\n",
        "!cp /content/metadata.csv /content/drive/MyDrive/vMalConv/metadata.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRilyqTXnrE"
      },
      "source": [
        "**Step 6:** Download and install Ember:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76bc7PEmlwKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7be75e8-91f9-483b-f03a-25ea0543fc44"
      },
      "source": [
        "!pip install git+https://github.com/PFGimenez/ember.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PFGimenez/ember.git\n",
            "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-k0ywb01r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-k0ywb01r\n",
            "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lief"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aRVMSwCQT7D",
        "outputId": "b734f3da-99f3-4489-a742-871f0e02dcda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lief in /usr/local/lib/python3.10/dist-packages (0.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXym5qd8Yv8f"
      },
      "source": [
        "**Step 7:** Read vectorized features from the data files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfcHyoTsmCFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cde7960-bc04-4de5-fd67-e8e256562328"
      },
      "source": [
        "import ember\n",
        "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"drive/MyDrive/vMalConv/\")\n",
        "metadata_dataframe = ember.read_metadata(\"drive/MyDrive/vMalConv/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTRCz7m7Z7EH"
      },
      "source": [
        "**Step 8:** Get rid of rows with no labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj63lcvin44q"
      },
      "source": [
        "labelrows = (y_train != -1)\n",
        "X_train = X_train[labelrows]\n",
        "y_train = y_train[labelrows]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVG59AGooyC5"
      },
      "source": [
        "import h5py\n",
        "h5f = h5py.File('X_train.h5', 'w')\n",
        "h5f.create_dataset('X_train', data=X_train)\n",
        "h5f.close()\n",
        "h5f = h5py.File('y_train.h5', 'w')\n",
        "h5f.create_dataset('y_train', data=y_train)\n",
        "h5f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tmUIJNvpZch"
      },
      "source": [
        "!cp /content/X_train.h5 /content/drive/MyDrive/vMalConv/X_train.h5\n",
        "!cp /content/y_train.h5 /content/drive/MyDrive/vMalConv/y_train.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional Step 8.5:** To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets. You can use the [Pandas Dataframe sample() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html), or come up with your own sampling methodology. Be mindful of the fact that the database is heavily imbalanced."
      ],
      "metadata": {
        "id": "tKoXSzp59RN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert arrays to DataFrame\n",
        "X_train_samp = pd.DataFrame(X_train)\n",
        "X_test_samp = pd.DataFrame(X_test)\n",
        "y_train_samp = pd.Series(y_train)\n",
        "y_test_samp = pd.Series(y_test)\n",
        "\n",
        "# Sample 5% of the training and test data based on class weights\n",
        "X_train_samp = X_train_samp.sample(frac=0.05, weights=y_train_samp, random_state=42)\n",
        "X_test_samp = X_test_samp.sample(frac=0.05, weights=y_test_samp, random_state=42)\n",
        "\n",
        "# Get corresponding labels\n",
        "y_train_samp = y_train_samp.loc[X_train_samp.index]\n",
        "y_test_samp = y_test_samp.loc[X_test_samp.index]\n",
        "\n",
        "# Convert back to arrays\n",
        "X_train_samp = X_train_samp.values\n",
        "X_test_samp = X_test_samp.values\n",
        "\n",
        "y_train_samp = y_train_samp.values\n",
        "y_test_samp = y_test_samp.values"
      ],
      "metadata": {
        "id": "-X9wwv_n9QkY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1bRlBWlaQdd"
      },
      "source": [
        "> **Task 1:** Complete the following code to build the architecture of MalConv in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1ZlKQwDv4uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17fc8d2-de57-4877-9903-ff14f515cf39"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MalConv(nn.Module):\n",
        "    def __init__(self, input_length=2000000, embedding_dim=8, window_size=128, output_dim=1):\n",
        "        super(MalConv, self).__init__()\n",
        "        self.embed = nn.Embedding(257, embedding_dim, padding_idx=0)  # 256 unique bytes, embedding dimension\n",
        "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=32, stride=32)\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=32, stride=32)\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x.clamp(min=0, max=256))  # Ensure indices are within the valid range\n",
        "        x = x.permute(0,3,1,2).squeeze()  # Conv1d expects (batch_size, channels, length)\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)  # Apply dropout after the first convolutional layer\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)  # Apply dropout after the second convolutional layer\n",
        "        x = torch.squeeze(torch.max(x, dim=2)[0])  # Global max pooling\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Example of creating a MalConv model\n",
        "input_length = 2000000  # The fixed length for each input file\n",
        "model = MalConv(input_length=input_length)\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MalConv(\n",
            "  (embed): Embedding(257, 8, padding_idx=0)\n",
            "  (conv1): Conv1d(8, 128, kernel_size=(32,), stride=(32,))\n",
            "  (conv2): Conv1d(128, 128, kernel_size=(32,), stride=(32,))\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihnLcFmbaet"
      },
      "source": [
        "**Step 8:** Partial fit the standardScaler to avoid overloading the memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4q5OfK9v9iN"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "mms = StandardScaler()\n",
        "for x in range(0,600000,100000):\n",
        "  mms.partial_fit(X_train[x:x+100000])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B33Oa1sTxdB0"
      },
      "source": [
        "X_train_samp = mms.transform(X_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vl5yrex0yY"
      },
      "source": [
        "## Reshape to create 3 channels ##\n",
        "import numpy as np\n",
        "X_train = np.reshape(X_train,(-1,1,2381))\n",
        "y_train = np.reshape(y_train,(-1,1,1))\n",
        "\n",
        "X_test = np.reshape(X_test,(-1,1,2381))\n",
        "y_test = np.reshape(y_test,(-1,1,1))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load, Tensorize, and Split** The following code takes care of converting the training data into Torch Tensors, and then splits it into 80% training and 20% validation datasets."
      ],
      "metadata": {
        "id": "b1iRXFtuvCps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming MalConv class definition is already provided as above\n",
        "\n",
        "# Convert your numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "# Split the data into training and validation sets (80% training, 20% validation)\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorDatasets and DataLoaders for training and validation sets\n",
        "train_dataset = TensorDataset(X_train_split, y_train_split)\n",
        "val_dataset = TensorDataset(X_val_split, y_val_split)\n",
        "\n",
        "batch_size = 64  # Adjust based on your GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Ja3fhJI6qJKN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zMgth6McCqV"
      },
      "source": [
        "> **Task 2:** Complete the following code to train the model on the GPU for 15 epochs, with a batch size of 64. If you are on Google Colab, don't forget to change the kernel in Runtime -> Change runtime type -> T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "# Initialize the MalConv model\n",
        "model = MalConv()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "\n",
        "# Directory to save model checkpoints\n",
        "save_dir = \"drive/MyDrive/vMalConv/\"\n",
        "\n",
        "# Training Loop with Validation\n",
        "num_epochs = 15  # Adjust the number of epochs as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.squeeze())\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
        "            val_loss += loss.item()\n",
        "    print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Model checkpoint saved to {checkpoint_path}')"
      ],
      "metadata": {
        "id": "iv7piF7dp0lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd8b6418-f136-46a8-ffd1-546c9537c14a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.19774708622644344\n",
            "Validation Loss: 0.14201460834046206\n",
            "Epoch 2, Training Loss: 0.14440490941678485\n",
            "Validation Loss: 0.12650665258467197\n",
            "Epoch 3, Training Loss: 0.128721279596289\n",
            "Validation Loss: 0.11641733775883913\n",
            "Epoch 4, Training Loss: 0.11882015326333542\n",
            "Validation Loss: 0.1116581153601408\n",
            "Epoch 5, Training Loss: 0.11229277927204967\n",
            "Validation Loss: 0.10791956521471341\n",
            "Model checkpoint saved to drive/MyDrive/vMalConv/model_epoch_5.pt\n",
            "Epoch 6, Training Loss: 0.10759670382216573\n",
            "Validation Loss: 0.10534268323133389\n",
            "Epoch 7, Training Loss: 0.10358501949347555\n",
            "Validation Loss: 0.10266330681194862\n",
            "Epoch 8, Training Loss: 0.10047864104968807\n",
            "Validation Loss: 0.10191622546315193\n",
            "Epoch 9, Training Loss: 0.09730843763152758\n",
            "Validation Loss: 0.10097109814385573\n",
            "Epoch 10, Training Loss: 0.09467848687171936\n",
            "Validation Loss: 0.09950316341420015\n",
            "Model checkpoint saved to drive/MyDrive/vMalConv/model_epoch_10.pt\n",
            "Epoch 11, Training Loss: 0.09278179765424381\n",
            "Validation Loss: 0.0994351504996419\n",
            "Epoch 12, Training Loss: 0.0902765502102673\n",
            "Validation Loss: 0.09881939102858305\n",
            "Epoch 13, Training Loss: 0.08850451260569195\n",
            "Validation Loss: 0.09823633260255059\n",
            "Epoch 14, Training Loss: 0.08758408532378574\n",
            "Validation Loss: 0.0976078837176164\n",
            "Epoch 15, Training Loss: 0.08601661990502228\n",
            "Validation Loss: 0.09942700398589174\n",
            "Model checkpoint saved to drive/MyDrive/vMalConv/model_epoch_15.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3:** Complete the following code to evaluate your trained model on the test data."
      ],
      "metadata": {
        "id": "obToo1WZtD4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Convert test data to PyTorch tensors\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create a TensorDataset and DataLoader for test data\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store model predictions and actual labels\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels_batch in test_loader:\n",
        "\n",
        "      # Move inputs to the appropriate device\n",
        "        inputs = inputs.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "      # Convert logits to probabilities using sigmoid activation\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "\n",
        "      # Convert probabilities to binary predictions (0 or 1)\n",
        "        predicted = (probabilities > 0.5).float()\n",
        "\n",
        "      # Store predictions and labels\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        labels.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "# Flatten nested arrays\n",
        "labels = np.array(labels).ravel()\n",
        "flat_predictions = np.array(predictions).ravel()\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "precision = precision_score(labels, predictions)\n",
        "recall = recall_score(labels, predictions)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')"
      ],
      "metadata": {
        "id": "83wqvS9jqppe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023b8c64-21e9-4d05-f9cd-5167a7810fc2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5231\n",
            "Precision: 0.5118\n",
            "Recall: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:** Comment on the results in this text box.\n",
        "\n",
        "The performance metrics provided indicate several important aspects of the model's performance, but they also raise some concerns:\n",
        "\n",
        "**Test Accuracy (0.5231)**: The test accuracy represents the proportion of correctly classified samples out of the total test set. In this case, the accuracy is approximately 52.31%, which means the model is slightly better than random guessing. However, this accuracy level might not be satisfactory, especially in the case of malware detection.\n",
        "\n",
        "**Precision (0.5118)**: Precision measures the proportion of true positive predictions out of all positive predictions made by the model. A precision of 0.5118 suggests that around 51.18% of the samples predicted as positive are actually positive. This indicates that the model's ability to avoid false positives is relatively low.\n",
        "\n",
        "**Recall (0.9999)**: Recall, also known as sensitivity, measures the proportion of true positive predictions out of all actual positive samples. A recall of 0.9999 indicates that the model is capturing almost all positive samples in the dataset. While high recall is desirable, such a high value, especially when combined with lower precision, could indicate potential issues, such as the model being overly sensitive and possibly prone to false positives.\n",
        "\n",
        "Overall, the model's performance appears to have a significant trade-off between precision and recall, where it tends to classify many samples as positive (either Malware or Benign) but struggles with precision, leading to a high false positive rate. This imbalance might be due to class imbalance in the dataset or model complexity.\n",
        "\n",
        "Further investigation into the model's behavior on different subsets of the data, analysis of misclassifications, and potential adjustments to the model architecture or training process could help improve its performance. Additionally, considering alternative evaluation metrics like F1-score, which combines precision and recall, might provide a more comprehensive understanding of the model's performance.\n",
        "\n",
        "With more time and resources allocated to this portion of the project, the model's performance can be significantly improved."
      ],
      "metadata": {
        "id": "W6fLYYxps91N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Post Training**: Once your model is trained, save and store the model. Then, create a function (or\n",
        "method) that takes a PE file as its argument, runs it through the trained model, and returns the\n",
        "output (i.e., Malware or Benign)."
      ],
      "metadata": {
        "id": "ytwujNxWP0LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/vMalConv/my_model.pth')"
      ],
      "metadata": {
        "id": "PHIiGHXDMPsa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo.anaconda.com/archive/Anaconda3-2020.02-Windows-x86_64.exe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltGXdUtBRu9E",
        "outputId": "86f86a2f-54b5-4759-a59a-812f71fff7a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-14 18:50:08--  https://repo.anaconda.com/archive/Anaconda3-2020.02-Windows-x86_64.exe\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 488908696 (466M) [application/octet-stream]\n",
            "Saving to: ‘Anaconda3-2020.02-Windows-x86_64.exe’\n",
            "\n",
            "Anaconda3-2020.02-W 100%[===================>] 466.26M   330MB/s    in 1.4s    \n",
            "\n",
            "2024-03-14 18:50:10 (330 MB/s) - ‘Anaconda3-2020.02-Windows-x86_64.exe’ saved [488908696/488908696]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def testPE(pe):\n",
        "  import ember\n",
        "  import numpy as np\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "  # Open downloaded PE file\n",
        "  testpe = open(pe, \"rb\").read()\n",
        "\n",
        "  # Extract Features\n",
        "  extract = ember.PEFeatureExtractor()\n",
        "  data = extract.feature_vector(testpe)\n",
        "  scaled_data = StandardScaler().transform(testpe)\n",
        "  X = np.reshape(scaled_data,(-1,1,2381))\n",
        "\n",
        "  model = MalConv()\n",
        "  model.load_state_dict(torch.load('/content/drive/MyDrive/vMalConv/my_model.pth'))\n",
        "  model.eval()\n",
        "  pred = model.predict_classes(X)\n",
        "\n",
        "  return pred"
      ],
      "metadata": {
        "id": "7A-xxq8lTYS5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testPE(\"Anaconda3-2020.02-Windows-x86_64.exe\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "K_6tOP_6WLeu",
        "outputId": "9a860205-d8f2-4abb-c3df-c9981ecc2b03"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'lief' has no attribute 'bad_format'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-105d0b852c3a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anaconda3-2020.02-Windows-x86_64.exe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-af34e79b830e>\u001b[0m in \u001b[0;36mtestPE\u001b[0;34m(pe)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Extract Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mextract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0member\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPEFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mscaled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2381\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ember/features.py\u001b[0m in \u001b[0;36mfeature_vector\u001b[0;34m(self, bytez)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytez\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_raw_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytez\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ember/features.py\u001b[0m in \u001b[0;36mraw_features\u001b[0;34m(self, bytez)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytez\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         lief_errors = (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, lief.read_out_of_bound,\n\u001b[0m\u001b[1;32m    538\u001b[0m                        RuntimeError)\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'lief' has no attribute 'bad_format'"
          ]
        }
      ]
    }
  ]
}